{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MonteCarlo.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "ioKXJC6k8bAj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "bf6746c3-5acb-475d-e4d5-6ab8c0391cec",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530753998693,
          "user_tz": 360,
          "elapsed": 263,
          "user": {
            "displayName": "Kay Park",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "115858547447612857691"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
        "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "# Note: you may need to update your version of future\n",
        "# sudo pip install -U future\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Grid: # Environment\n",
        "  def __init__(self, width, height, start):\n",
        "    self.width = width\n",
        "    self.height = height\n",
        "    self.i = start[0]\n",
        "    self.j = start[1]\n",
        "\n",
        "  def set(self, rewards, actions):\n",
        "    # rewards should be a dict of: (i, j): r (row, col): reward\n",
        "    # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
        "    self.rewards = rewards\n",
        "    self.actions = actions\n",
        "\n",
        "  def set_state(self, s):\n",
        "    self.i = s[0]\n",
        "    self.j = s[1]\n",
        "\n",
        "  def current_state(self):\n",
        "    return (self.i, self.j)\n",
        "\n",
        "  def is_terminal(self, s):\n",
        "    return s not in self.actions\n",
        "\n",
        "  def move(self, action):\n",
        "    # check if legal move first\n",
        "    if action in self.actions[(self.i, self.j)]:\n",
        "      if action == 'U':\n",
        "        self.i -= 1\n",
        "      elif action == 'D':\n",
        "        self.i += 1\n",
        "      elif action == 'R':\n",
        "        self.j += 1\n",
        "      elif action == 'L':\n",
        "        self.j -= 1\n",
        "    # return a reward (if any)\n",
        "    return self.rewards.get((self.i, self.j), 0)\n",
        "\n",
        "  def undo_move(self, action):\n",
        "    # these are the opposite of what U/D/L/R should normally do\n",
        "    if action == 'U':\n",
        "      self.i += 1\n",
        "    elif action == 'D':\n",
        "      self.i -= 1\n",
        "    elif action == 'R':\n",
        "      self.j -= 1\n",
        "    elif action == 'L':\n",
        "      self.j += 1\n",
        "    # raise an exception if we arrive somewhere we shouldn't be\n",
        "    # should never happen\n",
        "    assert(self.current_state() in self.all_states())\n",
        "\n",
        "  def game_over(self):\n",
        "    # returns true if game is over, else false\n",
        "    # true if we are in a state where no actions are possible\n",
        "    return (self.i, self.j) not in self.actions\n",
        "\n",
        "  def all_states(self):\n",
        "    # possibly buggy but simple way to get all states\n",
        "    # either a position that has possible next actions\n",
        "    # or a position that yields a reward\n",
        "    return set(self.actions.keys()) | set(self.rewards.keys())\n",
        "\n",
        "\n",
        "def standard_grid():\n",
        "  # define a grid that describes the reward for arriving at each state\n",
        "  # and possible actions at each state\n",
        "  # the grid looks like this\n",
        "  # x means you can't go there\n",
        "  # s means start position\n",
        "  # number means reward at that state\n",
        "  # .  .  .  1\n",
        "  # .  x  . -1\n",
        "  # s  .  .  .\n",
        "  g = Grid(3, 4, (2, 0))\n",
        "  rewards = {(0, 3): 1, (1, 3): -1}\n",
        "  actions = {\n",
        "    (0, 0): ('D', 'R'),\n",
        "    (0, 1): ('L', 'R'),\n",
        "    (0, 2): ('L', 'D', 'R'),\n",
        "    (1, 0): ('U', 'D'),\n",
        "    (1, 2): ('U', 'D', 'R'),\n",
        "    (2, 0): ('U', 'R'),\n",
        "    (2, 1): ('L', 'R'),\n",
        "    (2, 2): ('L', 'R', 'U'),\n",
        "    (2, 3): ('L', 'U'),\n",
        "  }\n",
        "  g.set(rewards, actions)\n",
        "  return g\n",
        "\n",
        "\n",
        "def negative_grid(step_cost=-0.1):\n",
        "  # in this game we want to try to minimize the number of moves\n",
        "  # so we will penalize every move\n",
        "  g = standard_grid()\n",
        "  g.rewards.update({\n",
        "    (0, 0): step_cost,\n",
        "    (0, 1): step_cost,\n",
        "    (0, 2): step_cost,\n",
        "    (1, 0): step_cost,\n",
        "    (1, 2): step_cost,\n",
        "    (2, 0): step_cost,\n",
        "    (2, 1): step_cost,\n",
        "    (2, 2): step_cost,\n",
        "    (2, 3): step_cost,\n",
        "  })\n",
        "  return g\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o1_JBbao8jFr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "MoteCarlo Prediction with only policy evaluation"
      ]
    },
    {
      "metadata": {
        "id": "Sggw247x6DQ4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "80343be7-1712-458c-d44d-b532cf116c3f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530753146302,
          "user_tz": 360,
          "elapsed": 426,
          "user": {
            "displayName": "Kay Park",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "115858547447612857691"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
        "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "# Note: you may need to update your version of future\n",
        "# sudo pip install -U future\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "SMALL_ENOUGH = 1e-3\n",
        "GAMMA = 0.9\n",
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
        "\n",
        "def print_values(V, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      v = V.get((i,j), 0)\n",
        "      if v >= 0:\n",
        "        print(\" %.2f|\" % v, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "def print_policy(P, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      a = P.get((i,j), ' ')\n",
        "      print(\"  %s  |\" % a, end=\"\")\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "# NOTE: this is only policy evaluation, not optimization\n",
        "\n",
        "def play_game(grid, policy):\n",
        "  # returns a list of states and corresponding returns\n",
        "\n",
        "  # reset game to start at a random position\n",
        "  # we need to do this, because given our current deterministic policy\n",
        "  # we would never end up at certain states, but we still want to measure their value\n",
        "  start_states = list(grid.actions.keys())\n",
        "  start_idx = np.random.choice(len(start_states))\n",
        "  grid.set_state(start_states[start_idx])\n",
        "\n",
        "  s = grid.current_state()\n",
        "  states_and_rewards = [(s, 0)] # list of tuples of (state, reward)\n",
        "  while not grid.game_over():\n",
        "    a = policy[s]\n",
        "    r = grid.move(a)\n",
        "    s = grid.current_state()\n",
        "    states_and_rewards.append((s, r))\n",
        "  # calculate the returns by working backwards from the terminal state\n",
        "  G = 0\n",
        "  states_and_returns = []\n",
        "  first = True\n",
        "  for s, r in reversed(states_and_rewards):\n",
        "    # the value of the terminal state is 0 by definition\n",
        "    # we should ignore the first state we encounter\n",
        "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
        "    if first:\n",
        "      first = False\n",
        "    else:\n",
        "      states_and_returns.append((s, G))\n",
        "    G = r + GAMMA*G\n",
        "  states_and_returns.reverse() # we want it to be in order of state visited\n",
        "  return states_and_returns\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # use the standard grid again (0 for every step) so that we can compare\n",
        "  # to iterative policy evaluation\n",
        "  grid = standard_grid()\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid)\n",
        "\n",
        "  # state -> action\n",
        "  policy = {\n",
        "    (2, 0): 'U',\n",
        "    (1, 0): 'U',\n",
        "    (0, 0): 'R',\n",
        "    (0, 1): 'R',\n",
        "    (0, 2): 'R',\n",
        "    (1, 2): 'R',\n",
        "    (2, 1): 'R',\n",
        "    (2, 2): 'R',\n",
        "    (2, 3): 'U',\n",
        "  }\n",
        "\n",
        "  # initialize V(s) and returns\n",
        "  V = {}\n",
        "  returns = {} # dictionary of state -> list of returns we've received\n",
        "  states = grid.all_states()\n",
        "  for s in states:\n",
        "    if s in grid.actions:\n",
        "      returns[s] = []\n",
        "    else:\n",
        "      # terminal state or state we can't otherwise get to\n",
        "      V[s] = 0\n",
        "\n",
        "  # repeat\n",
        "  for t in range(100):\n",
        "\n",
        "    # generate an episode using pi\n",
        "    states_and_returns = play_game(grid, policy)\n",
        "    seen_states = set()\n",
        "    for s, G in states_and_returns:\n",
        "      # check if we have already seen s\n",
        "      # called \"first-visit\" MC policy evaluation\n",
        "      if s not in seen_states:\n",
        "        returns[s].append(G)\n",
        "        V[s] = np.mean(returns[s])\n",
        "        seen_states.add(s)\n",
        "\n",
        "  print(\"values:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"policy:\")\n",
        "  print_policy(policy, grid)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rewards:\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00|-1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "values:\n",
            "---------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.73| 0.00|-1.00| 0.00|\n",
            "---------------------------\n",
            " 0.66|-0.81|-0.90|-1.00|\n",
            "policy:\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  R  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  R  |  U  |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lTWIkPFi8lN6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "MonteCarlo plicy evaluation with optimization"
      ]
    },
    {
      "metadata": {
        "id": "QcWMDQPQ8ZO0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "outputId": "f8855f1b-a622-4c44-d761-6e01738bdedf",
        "executionInfo": {
          "status": "error",
          "timestamp": 1530754165680,
          "user_tz": 360,
          "elapsed": 154781,
          "user": {
            "displayName": "Kay Park",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "115858547447612857691"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
        "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "# Note: you may need to update your version of future\n",
        "# sudo pip install -U future\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "SMALL_ENOUGH = 1e-3\n",
        "GAMMA = 0.9\n",
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
        "\n",
        "def print_values(V, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      v = V.get((i,j), 0)\n",
        "      if v >= 0:\n",
        "        print(\" %.2f|\" % v, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "def print_policy(P, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      a = P.get((i,j), ' ')\n",
        "      print(\"  %s  |\" % a, end=\"\")\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "\n",
        "def play_game(grid, policy):\n",
        "  # returns a list of states and corresponding returns\n",
        "\n",
        "  # reset game to start at a random position\n",
        "  # we need to do this, because given our current deterministic policy\n",
        "  # we would never end up at certain states, but we still want to measure their value\n",
        "  start_states = list(grid.actions.keys())\n",
        "  start_idx = np.random.choice(len(start_states))\n",
        "  grid.set_state(start_states[start_idx])\n",
        "\n",
        "  s = grid.current_state()\n",
        "  states_and_rewards = [(s, 0)] # list of tuples of (state, reward)\n",
        "  while not grid.game_over():\n",
        "    a = policy[s]\n",
        "    r = grid.move(a)\n",
        "    s = grid.current_state()\n",
        "    states_and_rewards.append((s, r))\n",
        "  # calculate the returns by working backwards from the terminal state\n",
        "  G = 0\n",
        "  states_and_returns = []\n",
        "  first = True\n",
        "  for s, r in reversed(states_and_rewards):\n",
        "    # the value of the terminal state is 0 by definition\n",
        "    # we should ignore the first state we encounter\n",
        "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
        "    if first:\n",
        "      first = False\n",
        "    else:\n",
        "      states_and_returns.append((s, G))\n",
        "    G = r + GAMMA*G\n",
        "  states_and_returns.reverse() # we want it to be in order of state visited\n",
        "  return states_and_returns\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # use the standard grid again (0 for every step) so that we can compare\n",
        "  # to iterative policy evaluation\n",
        "  grid = standard_grid()\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid)\n",
        "\n",
        "  # state -> action\n",
        "  policy = {\n",
        "    (2, 0): 'U',\n",
        "    (1, 0): 'U',\n",
        "    (0, 0): 'R',\n",
        "    (0, 1): 'R',\n",
        "    (0, 2): 'R',\n",
        "    (1, 2): 'R',\n",
        "    (2, 1): 'R',\n",
        "    (2, 2): 'R',\n",
        "    (2, 3): 'L',\n",
        "  }\n",
        "\n",
        "  # initialize V(s) and returns\n",
        "  V = {}\n",
        "  returns = {} # dictionary of state -> list of returns we've received\n",
        "  states = grid.all_states()\n",
        "  for s in states:\n",
        "    if s in grid.actions:\n",
        "      returns[s] = []\n",
        "    else:\n",
        "      # terminal state or state we can't otherwise get to\n",
        "      V[s] = 0\n",
        "\n",
        "  # repeat\n",
        "  for t in range(100):\n",
        "\n",
        "    # generate an episode using pi\n",
        "    states_and_returns = play_game(grid, policy)\n",
        "    seen_states = set()\n",
        "    for s, G in states_and_returns:\n",
        "      # check if we have already seen s\n",
        "      # called \"first-visit\" MC policy evaluation\n",
        "      if s not in seen_states:\n",
        "        returns[s].append(G)\n",
        "        V[s] = np.mean(returns[s])\n",
        "        seen_states.add(s)\n",
        "\n",
        "  print(\"values:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"policy:\")\n",
        "  print_policy(policy, grid)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rewards:\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00|-1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-84eb6ecc8193>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;31m# generate an episode using pi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mstates_and_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0mseen_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstates_and_returns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-84eb6ecc8193>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(grid, policy)\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mstates_and_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-354d04d4046d>\u001b[0m in \u001b[0;36mmove\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# return a reward (if any)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mundo_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "4Fl1aCkS8rMV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "MonteCarlo Exploring Starts"
      ]
    },
    {
      "metadata": {
        "id": "jz1hTooy-np0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 1044
        },
        "outputId": "b5daa24d-6632-406f-a45e-8c02c7516ede",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530753260175,
          "user_tz": 360,
          "elapsed": 963,
          "user": {
            "displayName": "Kay Park",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "115858547447612857691"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
        "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "# Note: you may need to update your version of future\n",
        "# sudo pip install -U future\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "GAMMA = 0.9\n",
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
        "\n",
        "def print_values(V, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      v = V.get((i,j), 0)\n",
        "      if v >= 0:\n",
        "        print(\" %.2f|\" % v, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "def print_policy(P, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      a = P.get((i,j), ' ')\n",
        "      print(\"  %s  |\" % a, end=\"\")\n",
        "    print(\"\")\n",
        "\n",
        "# NOTE: this script implements the Monte Carlo Exploring-Starts method\n",
        "#       for finding the optimal policy\n",
        "\n",
        "def play_game(grid, policy):\n",
        "  # returns a list of states and corresponding returns\n",
        "\n",
        "  # reset game to start at a random position\n",
        "  # we need to do this if we have a deterministic policy\n",
        "  # we would never end up at certain states, but we still want to measure their value\n",
        "  # this is called the \"exploring starts\" method\n",
        "  start_states = list(grid.actions.keys())\n",
        "  start_idx = np.random.choice(len(start_states))\n",
        "  grid.set_state(start_states[start_idx])\n",
        "\n",
        "  s = grid.current_state()\n",
        "  a = np.random.choice(ALL_POSSIBLE_ACTIONS) # first action is uniformly random\n",
        "\n",
        "  # be aware of the timing\n",
        "  # each triple is s(t), a(t), r(t)\n",
        "  # but r(t) results from taking action a(t-1) from s(t-1) and landing in s(t)\n",
        "  states_actions_rewards = [(s, a, 0)]\n",
        "  seen_states = set()\n",
        "  while True:\n",
        "    old_s = grid.current_state()\n",
        "    r = grid.move(a)\n",
        "    s = grid.current_state()\n",
        "\n",
        "    if s in seen_states:\n",
        "      # hack so that we don't end up in an infinitely long episode\n",
        "      # bumping into the wall repeatedly\n",
        "      states_actions_rewards.append((s, None, -100))\n",
        "      break\n",
        "    elif grid.game_over():\n",
        "      states_actions_rewards.append((s, None, r))\n",
        "      break\n",
        "    else:\n",
        "      a = policy[s]\n",
        "      states_actions_rewards.append((s, a, r))\n",
        "    seen_states.add(s)\n",
        "\n",
        "  # calculate the returns by working backwards from the terminal state\n",
        "  G = 0\n",
        "  states_actions_returns = []\n",
        "  first = True\n",
        "  for s, a, r in reversed(states_actions_rewards):\n",
        "    # the value of the terminal state is 0 by definition\n",
        "    # we should ignore the first state we encounter\n",
        "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
        "    if first:\n",
        "      first = False\n",
        "    else:\n",
        "      states_actions_returns.append((s, a, G))\n",
        "    G = r + GAMMA*G\n",
        "  states_actions_returns.reverse() # we want it to be in order of state visited\n",
        "  return states_actions_returns\n",
        "\n",
        "\n",
        "def max_dict(d):\n",
        "  # returns the argmax (key) and max (value) from a dictionary\n",
        "  # put this into a function since we are using it so often\n",
        "  max_key = None\n",
        "  max_val = float('-inf')\n",
        "  for k, v in d.items():\n",
        "    if v > max_val:\n",
        "      max_val = v\n",
        "      max_key = k\n",
        "  return max_key, max_val\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # use the standard grid again (0 for every step) so that we can compare\n",
        "  # to iterative policy evaluation\n",
        "  # grid = standard_grid()\n",
        "  # try the negative grid too, to see if agent will learn to go past the \"bad spot\"\n",
        "  # in order to minimize number of steps\n",
        "  grid = negative_grid(step_cost=-0.9)\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid)\n",
        "\n",
        "  # state -> action\n",
        "  # initialize a random policy\n",
        "  policy = {}\n",
        "  for s in grid.actions.keys():\n",
        "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
        "\n",
        "  # initialize Q(s,a) and returns\n",
        "  Q = {}\n",
        "  returns = {} # dictionary of state -> list of returns we've received\n",
        "  states = grid.all_states()\n",
        "  for s in states:\n",
        "    if s in grid.actions: # not a terminal state\n",
        "      Q[s] = {}\n",
        "      for a in ALL_POSSIBLE_ACTIONS:\n",
        "        Q[s][a] = 0 # needs to be initialized to something so we can argmax it\n",
        "        returns[(s,a)] = []\n",
        "    else:\n",
        "      # terminal state or state we can't otherwise get to\n",
        "      pass\n",
        "\n",
        "  # repeat until convergence\n",
        "  deltas = []\n",
        "  for t in range(2000):\n",
        "    if t % 100 == 0:\n",
        "      print(t)\n",
        "\n",
        "    # generate an episode using pi\n",
        "    biggest_change = 0\n",
        "    states_actions_returns = play_game(grid, policy)\n",
        "    seen_state_action_pairs = set()\n",
        "    for s, a, G in states_actions_returns:\n",
        "      # check if we have already seen s\n",
        "      # called \"first-visit\" MC policy evaluation\n",
        "      sa = (s, a)\n",
        "      if sa not in seen_state_action_pairs:\n",
        "        old_q = Q[s][a]\n",
        "        returns[sa].append(G)\n",
        "        Q[s][a] = np.mean(returns[sa])\n",
        "        biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
        "        seen_state_action_pairs.add(sa)\n",
        "    deltas.append(biggest_change)\n",
        "\n",
        "    # update policy\n",
        "    for s in policy.keys():\n",
        "      policy[s] = max_dict(Q[s])[0]\n",
        "\n",
        "  plt.plot(deltas)\n",
        "  plt.show()\n",
        "\n",
        "  print(\"final policy:\")\n",
        "  print_policy(policy, grid)\n",
        "\n",
        "  # find V\n",
        "  V = {}\n",
        "  for s, Qs in Q.items():\n",
        "    V[s] = max_dict(Q[s])[1]\n",
        "\n",
        "  print(\"final values:\")\n",
        "  print_values(V, grid)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rewards:\n",
            "---------------------------\n",
            "-0.90|-0.90|-0.90| 1.00|\n",
            "---------------------------\n",
            "-0.90| 0.00|-0.90|-1.00|\n",
            "---------------------------\n",
            "-0.90|-0.90|-0.90|-0.90|\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAFKCAYAAADMuCxnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f208ff43c88>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "final policy:\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  R  |     |\n",
            "---------------------------\n",
            "  R  |  R  |  U  |  U  |\n",
            "final values:\n",
            "---------------------------\n",
            "-1.06|-0.15| 1.00| 0.00|\n",
            "---------------------------\n",
            "-2.02| 0.00|-1.00| 0.00|\n",
            "---------------------------\n",
            "-3.17|-2.67|-1.80|-1.00|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i4UgKoZh-3xu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "MonteCarlo Without Exploring Starts"
      ]
    },
    {
      "metadata": {
        "id": "7s3nKknJ6DRI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 789
        },
        "outputId": "2b357eaa-8be4-4180-f663-e9a9d7b8ee4f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530753355778,
          "user_tz": 360,
          "elapsed": 3681,
          "user": {
            "displayName": "Kay Park",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "115858547447612857691"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
        "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "# Note: you may need to update your version of future\n",
        "# sudo pip install -U future\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#from grid_world import standard_grid, negative_grid\n",
        "#from iterative_policy_evaluation import print_values, print_policy\n",
        "#from monte_carlo_es import max_dict\n",
        "\n",
        "GAMMA = 0.9\n",
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
        "\n",
        "def print_values(V, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      v = V.get((i,j), 0)\n",
        "      if v >= 0:\n",
        "        print(\" %.2f|\" % v, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "def print_policy(P, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      a = P.get((i,j), ' ')\n",
        "      print(\"  %s  |\" % a, end=\"\")\n",
        "    print(\"\")\n",
        "\n",
        "def max_dict(d):\n",
        "  # returns the argmax (key) and max (value) from a dictionary\n",
        "  # put this into a function since we are using it so often\n",
        "  max_key = None\n",
        "  max_val = float('-inf')\n",
        "  for k, v in d.items():\n",
        "    if v > max_val:\n",
        "      max_val = v\n",
        "      max_key = k\n",
        "  return max_key, max_val\n",
        "\n",
        "# NOTE: find optimal policy and value function\n",
        "#       using on-policy first-visit MC\n",
        "\n",
        "def random_action(a, eps=0.1):\n",
        "  # choose given a with probability 1 - eps + eps/4\n",
        "  # choose some other a' != a with probability eps/4\n",
        "  p = np.random.random()\n",
        "  # if p < (1 - eps + eps/len(ALL_POSSIBLE_ACTIONS)):\n",
        "  #   return a\n",
        "  # else:\n",
        "  #   tmp = list(ALL_POSSIBLE_ACTIONS)\n",
        "  #   tmp.remove(a)\n",
        "  #   return np.random.choice(tmp)\n",
        "  #\n",
        "  # this is equivalent to the above\n",
        "  if p < (1 - eps):\n",
        "    return a\n",
        "  else:\n",
        "    return np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
        "\n",
        "def play_game(grid, policy):\n",
        "  # returns a list of states and corresponding returns\n",
        "  # in this version we will NOT use \"exploring starts\" method\n",
        "  # instead we will explore using an epsilon-soft policy\n",
        "  s = (2, 0)\n",
        "  grid.set_state(s)\n",
        "  a = random_action(policy[s])\n",
        "\n",
        "  # be aware of the timing\n",
        "  # each triple is s(t), a(t), r(t)\n",
        "  # but r(t) results from taking action a(t-1) from s(t-1) and landing in s(t)\n",
        "  states_actions_rewards = [(s, a, 0)]\n",
        "  while True:\n",
        "    r = grid.move(a)\n",
        "    s = grid.current_state()\n",
        "    if grid.game_over():\n",
        "      states_actions_rewards.append((s, None, r))\n",
        "      break\n",
        "    else:\n",
        "      a = random_action(policy[s]) # the next state is stochastic\n",
        "      states_actions_rewards.append((s, a, r))\n",
        "\n",
        "  # calculate the returns by working backwards from the terminal state\n",
        "  G = 0\n",
        "  states_actions_returns = []\n",
        "  first = True\n",
        "  for s, a, r in reversed(states_actions_rewards):\n",
        "    # the value of the terminal state is 0 by definition\n",
        "    # we should ignore the first state we encounter\n",
        "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
        "    if first:\n",
        "      first = False\n",
        "    else:\n",
        "      states_actions_returns.append((s, a, G))\n",
        "    G = r + GAMMA*G\n",
        "  states_actions_returns.reverse() # we want it to be in order of state visited\n",
        "  return states_actions_returns\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # use the standard grid again (0 for every step) so that we can compare\n",
        "  # to iterative policy evaluation\n",
        "  # grid = standard_grid()\n",
        "  # try the negative grid too, to see if agent will learn to go past the \"bad spot\"\n",
        "  # in order to minimize number of steps\n",
        "  grid = negative_grid(step_cost=-0.1)\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid)\n",
        "\n",
        "  # state -> action\n",
        "  # initialize a random policy\n",
        "  policy = {}\n",
        "  for s in grid.actions.keys():\n",
        "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
        "\n",
        "  # initialize Q(s,a) and returns\n",
        "  Q = {}\n",
        "  returns = {} # dictionary of state -> list of returns we've received\n",
        "  states = grid.all_states()\n",
        "  for s in states:\n",
        "    if s in grid.actions: # not a terminal state\n",
        "      Q[s] = {}\n",
        "      for a in ALL_POSSIBLE_ACTIONS:\n",
        "        Q[s][a] = 0\n",
        "        returns[(s,a)] = []\n",
        "    else:\n",
        "      # terminal state or state we can't otherwise get to\n",
        "      pass\n",
        "\n",
        "  # repeat until convergence\n",
        "  deltas = []\n",
        "  for t in range(5000):\n",
        "    if t % 1000 == 0:\n",
        "      print(t)\n",
        "\n",
        "    # generate an episode using pi\n",
        "    biggest_change = 0\n",
        "    states_actions_returns = play_game(grid, policy)\n",
        "\n",
        "    # calculate Q(s,a)\n",
        "    seen_state_action_pairs = set()\n",
        "    for s, a, G in states_actions_returns:\n",
        "      # check if we have already seen s\n",
        "      # called \"first-visit\" MC policy evaluation\n",
        "      sa = (s, a)\n",
        "      if sa not in seen_state_action_pairs:\n",
        "        old_q = Q[s][a]\n",
        "        returns[sa].append(G)\n",
        "        Q[s][a] = np.mean(returns[sa])\n",
        "        biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
        "        seen_state_action_pairs.add(sa)\n",
        "    deltas.append(biggest_change)\n",
        "\n",
        "    # calculate new policy pi(s) = argmax[a]{ Q(s,a) }\n",
        "    for s in policy.keys():\n",
        "      a, _ = max_dict(Q[s])\n",
        "      policy[s] = a\n",
        "\n",
        "  plt.plot(deltas)\n",
        "  plt.show()\n",
        "\n",
        "  # find the optimal state-value function\n",
        "  # V(s) = max[a]{ Q(s,a) }\n",
        "  V = {}\n",
        "  for s in policy.keys():\n",
        "    V[s] = max_dict(Q[s])[1]\n",
        "\n",
        "  print(\"final values:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"final policy:\")\n",
        "  print_policy(policy, grid)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rewards:\n",
            "---------------------------\n",
            "-0.10|-0.10|-0.10| 1.00|\n",
            "---------------------------\n",
            "-0.10| 0.00|-0.10|-1.00|\n",
            "---------------------------\n",
            "-0.10|-0.10|-0.10|-0.10|\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFKCAYAAAAqkecjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f2090747390>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "final values:\n",
            "---------------------------\n",
            "-0.06| 0.78| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.10| 0.00| 0.78| 0.00|\n",
            "---------------------------\n",
            " 0.22| 0.37| 0.54| 0.36|\n",
            "final policy:\n",
            "---------------------------\n",
            "  D  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  D  |     |  U  |     |\n",
            "---------------------------\n",
            "  R  |  R  |  U  |  L  |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2I6Mtd4bAV2k",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}